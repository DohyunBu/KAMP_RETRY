{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# All in one"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import copy\n",
    "import os\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import f1_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train_Val_Test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"./preprocess_data.csv\") \n",
    "quality = pd.read_csv(\"./preprocess_quality.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_qual, val_test_qual, _, _ = train_test_split(quality,quality,test_size=0.4,stratify=quality['불량단계'], random_state=7)\n",
    "val_qual, test_qual, _, _ = train_test_split(val_test_qual,val_test_qual,test_size=0.5,stratify=val_test_qual['불량단계'], random_state=19)\n",
    "quality = pd.concat([train_qual,val_qual,test_qual]).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_len = len(data.columns[1:])\n",
    "selected_len = 19\n",
    "pca = PCA(n_components=selected_len)\n",
    "\n",
    "train_data = pd.DataFrame([], columns=data.columns)\n",
    "for i in train_qual['배정번호']:\n",
    "    train_data = pd.concat([train_data, data[data['배정번호']==i]]).reset_index(drop=True)\n",
    "\n",
    "pca.fit(train_data.loc[:,data.columns[1:]])\n",
    "data.loc[:,data.columns[1:1+selected_len]] = pca.transform(data.loc[:,data.columns[1:]])\n",
    "data = data.iloc[:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_quality = quality.iloc[:-len(quality)*2//5]\n",
    "for cluster_idx in tqdm(batch_quality['배정번호']):\n",
    "    exec(f\"torch_data{cluster_idx} = torch.tensor(np.array(data.loc[data['배정번호']==cluster_idx, data.columns[1:1+selected_len]]))\")\n",
    "    exec(f\"num{cluster_idx} = batch_quality[batch_quality['배정번호']==cluster_idx]['위험군개수'].iloc[0]\")\n",
    "    exec(f\"torch_data{cluster_idx} = torch_data{cluster_idx}.type(torch.float32)\")\n",
    "        \n",
    "\n",
    "batch_quality_val = quality.iloc[-len(quality)*2//5:-len(quality)//5]\n",
    "for cluster_idx in tqdm(batch_quality_val['배정번호']):\n",
    "    exec(f\"torch_data{cluster_idx} = torch.tensor(np.array(data.loc[data['배정번호']==cluster_idx, data.columns[1:1+selected_len]]))\")\n",
    "    exec(f\"num{cluster_idx} = batch_quality_val[batch_quality_val['배정번호']==cluster_idx]['위험군개수'].iloc[0]\")\n",
    "    exec(f\"torch_data{cluster_idx} = torch_data{cluster_idx}.type(torch.float32)\")\n",
    "\n",
    "\n",
    "batch_quality_test = quality.iloc[-len(quality)//5:]\n",
    "for cluster_idx in tqdm(batch_quality_test['배정번호']):\n",
    "    exec(f\"torch_data{cluster_idx} = torch.tensor(np.array(data.loc[data['배정번호']==cluster_idx, data.columns[1:1+selected_len]]))\")\n",
    "    exec(f\"num{cluster_idx} = batch_quality_test[batch_quality_test['배정번호']==cluster_idx]['위험군개수'].iloc[0]\")\n",
    "    exec(f\"torch_data{cluster_idx} = torch_data{cluster_idx}.type(torch.float32)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_num = 64\n",
    "\n",
    "def cohesion(group1_data):\n",
    "    return torch.cdist(group1_data, group1_data.mean(0).reshape(-1,hidden_num)).mean() \n",
    "\n",
    "def classify_loss(group1_logit_data, group2_logit_data, num):\n",
    "    accuracy_penalty = 1\n",
    "    if (num/(len(group1_logit_data) + len(group2_logit_data)) >= 0.0004525*10) and (len(group2_logit_data)/(len(group1_logit_data) + len(group2_logit_data)) < 0.0004525*10):\n",
    "        accuracy_penalty = 20 \n",
    "    elif (num/(len(group1_logit_data) + len(group2_logit_data)) < 0.0004525*10) and (len(group2_logit_data)/(len(group1_logit_data) + len(group2_logit_data)) >= 0.0004525*10):\n",
    "        accuracy_penalty = 10 \n",
    "    else:\n",
    "        accuracy_penalty = 1\n",
    "      \n",
    "\n",
    "    if len(group2_logit_data) >= num:\n",
    "        values, indices = torch.sort(group1_logit_data)\n",
    "        group1_values = values\n",
    "        values, indices = torch.sort(group2_logit_data)\n",
    "        group2_values = values\n",
    "\n",
    "        if len(group1_values) and len(group2_values):\n",
    "            return (-torch.log(group1_values).mean() + torch.log(1 - group2_values).mean())*(torch.abs(torch.tensor((len(group2_logit_data) - num)/(len(group1_logit_data)+len(group2_logit_data)))))*accuracy_penalty\n",
    "        elif not len(group1_values):\n",
    "            return torch.log(1 - group2_values).mean()*(torch.abs(torch.tensor((len(group2_logit_data) - num)/(len(group1_logit_data)+len(group2_logit_data)))))*accuracy_penalty \n",
    "        else:\n",
    "            return -torch.log(group1_values).mean()*(torch.abs(torch.tensor((len(group2_logit_data) - num)/(len(group1_logit_data)+len(group2_logit_data)))))*accuracy_penalty\n",
    "    \n",
    "    else:\n",
    "        values, indices = torch.sort(group1_logit_data)\n",
    "        group1_values = values\n",
    "        values, indices = torch.sort(group2_logit_data)\n",
    "        group2_values = values\n",
    "        if len(group1_values) and len(group2_values):\n",
    "            return (torch.log(group1_values).mean() - torch.log(1 - group2_values).mean())*(torch.abs(torch.tensor((len(group2_logit_data) - num)/(len(group1_logit_data)+len(group2_logit_data)))))*accuracy_penalty \n",
    "        elif not len(group1_values):\n",
    "            return - torch.log(1 - group2_values).mean()*(torch.abs(torch.tensor((len(group2_logit_data) - num)/(len(group1_logit_data)+len(group2_logit_data)))))*accuracy_penalty \n",
    "        else:\n",
    "            return torch.log(group1_values).mean()*(torch.abs(torch.tensor((len(group2_logit_data) - num)/(len(group1_logit_data)+len(group2_logit_data)))))*accuracy_penalty \n",
    "\n",
    "\n",
    "def clustering_loss(group1_data, group2_data, group1_logit_data, group2_logit_data, num, alpha, gamma):\n",
    "    return alpha * cohesion(group1_data) + gamma * classify_loss(group1_logit_data, group2_logit_data, num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def val_isNG(group1_data, group2_data):\n",
    "    if ((len(group2_data) / (len(group1_data)+len(group2_data))) >= 0.0004525*10):\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def true_isNG(group1_data, group2_data, num):\n",
    "    if ((num / (len(group1_data)+len(group2_data))) >= 0.0004525*10):\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Cluster_model(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Cluster_model, self).__init__()\n",
    "        self.fe = nn.Linear(selected_len, hidden_num)\n",
    "        self.hidden1 = nn.Linear(hidden_num,hidden_num)\n",
    "        self.hidden2 = nn.Linear(hidden_num,hidden_num)\n",
    "        self.hidden3 = nn.Linear(hidden_num,hidden_num)\n",
    "        self.latent = nn.Linear(hidden_num, hidden_num)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.last = nn.Linear(hidden_num, 1)\n",
    "        self.sig = nn.Sigmoid()\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fe(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.hidden1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.hidden2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.hidden3(x)\n",
    "        x = self.relu(x)\n",
    "        f = self.latent(x)\n",
    "        x = self.last(f)\n",
    "        x = self.sig(x)\n",
    "        return f, x\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 3000\n",
    "best_loss = 1000000\n",
    "best_f1_score = 0\n",
    "best_model = None   \n",
    "\n",
    "device = 'cpu'\n",
    "model = Cluster_model().to(device)\n",
    "lr = 1e-4\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "for epoch in tqdm(range(epochs)):\n",
    "    loss = 0\n",
    "    rand_i_list = random.sample(range(len(batch_quality['배정번호'])), k=10)\n",
    "    optimizer.zero_grad()\n",
    "    for i in range(10):\n",
    "        rand_i = rand_i_list[i]\n",
    "        cluster_idx = np.array(batch_quality['배정번호'])[rand_i]\n",
    "\n",
    "        model.train()\n",
    "        \n",
    "        exec(f\"f, y_hat = model(torch_data{cluster_idx})\")\n",
    "        loss += clustering_loss(f[np.array(y_hat > 0.5).reshape(-1)], f[np.array(y_hat <= 0.5).reshape(-1)], y_hat[np.array(y_hat > 0.5).reshape(-1)], y_hat[np.array(y_hat <= 0.5).reshape(-1)], eval(f\"num{cluster_idx}\"), 1, 10000)\n",
    "    loss /= 10\n",
    "    \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if epoch % 10 == 9:\n",
    "        val_loss = 0\n",
    "        model.eval()\n",
    "        pred_isNG_list = []\n",
    "        true_isNG_list = []\n",
    "        \n",
    "        for cluster_idx in batch_quality_val['배정번호']:    \n",
    "            exec(f\"f, y_hat = model(torch_data{cluster_idx})\")\n",
    "            pred_isNG_list.append(val_isNG(f[np.array(y_hat > 0.5).reshape(-1)], f[np.array(y_hat <= 0.5).reshape(-1)]))\n",
    "            true_isNG_list.append(true_isNG(f[np.array(y_hat > 0.5).reshape(-1)], f[np.array(y_hat <= 0.5).reshape(-1)], eval(f\"num{cluster_idx}\")))\n",
    "           \n",
    "        val_f1_score = f1_score(np.array(true_isNG_list).reshape(-1,1), np.array(pred_isNG_list).reshape(-1,1), average='macro')\n",
    "        if best_f1_score < val_f1_score:\n",
    "            best_f1_score = val_f1_score\n",
    "            best_model = copy.deepcopy(model)\n",
    "            torch.save(best_model,\"best_model.pt\")\n",
    "            \n",
    "            print(f\"[epoch {epoch}] best_f1 : {best_f1_score}\")\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load('best_model.pt')\n",
    "model.eval()\n",
    "\n",
    "total_val_label = []\n",
    "for cluster_idx in batch_quality_val['배정번호']:    \n",
    "    exec(f\"f, y_hat = model(torch_data{cluster_idx})\")\n",
    "    temp_label = np.array([0 for i in range(len(y_hat))])\n",
    "    temp_label[np.array(y_hat <= 0.5).reshape(-1)] = 1\n",
    "    total_val_label.append(temp_label.tolist())\n",
    "\n",
    "total_test_label = []\n",
    "for cluster_idx in batch_quality_test['배정번호']:    \n",
    "    exec(f\"f, y_hat = model(torch_data{cluster_idx})\")\n",
    "    temp_label = np.array([0 for i in range(len(y_hat))])\n",
    "    temp_label[np.array(y_hat <= 0.5).reshape(-1)] = 1\n",
    "    total_test_label.append(temp_label.tolist())\n",
    "\n",
    "total_train_label = []\n",
    "for cluster_idx in batch_quality['배정번호']:    \n",
    "    exec(f\"f, y_hat = model(torch_data{cluster_idx})\")\n",
    "    temp_label = np.array([0 for i in range(len(y_hat))])\n",
    "    temp_label[np.array(y_hat <= 0.5).reshape(-1)] = 1\n",
    "    total_train_label.append(temp_label.tolist())\n",
    "\n",
    "total_label = total_train_label + total_val_label + total_test_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load('best_model.pt')\n",
    "model.eval()\n",
    "\n",
    "pred_isNG_list = []\n",
    "true_isNG_list = []\n",
    "\n",
    "for cluster_idx in batch_quality_val['배정번호']:    \n",
    "    exec(f\"f, y_hat = model(torch_data{cluster_idx})\")\n",
    "    pred_isNG_list.append(val_isNG(f[np.array(y_hat > 0.5).reshape(-1)], f[np.array(y_hat <= 0.5).reshape(-1)]))\n",
    "    true_isNG_list.append(true_isNG(f[np.array(y_hat > 0.5).reshape(-1)], f[np.array(y_hat <= 0.5).reshape(-1)], eval(f\"num{cluster_idx}\")))\n",
    "   \n",
    "val_f1_score = f1_score(np.array(true_isNG_list).reshape(-1,1), np.array(pred_isNG_list).reshape(-1,1),average='macro')\n",
    "\n",
    "pred_isNG_list = []\n",
    "true_isNG_list = []\n",
    "\n",
    "for cluster_idx in batch_quality_test['배정번호']:    \n",
    "    exec(f\"f, y_hat = model(torch_data{cluster_idx})\")\n",
    "    pred_isNG_list.append(val_isNG(f[np.array(y_hat > 0.5).reshape(-1)], f[np.array(y_hat <= 0.5).reshape(-1)]))\n",
    "    true_isNG_list.append(true_isNG(f[np.array(y_hat > 0.5).reshape(-1)], f[np.array(y_hat <= 0.5).reshape(-1)], eval(f\"num{cluster_idx}\")))\n",
    "   \n",
    "test_f1_score = f1_score(np.array(true_isNG_list).reshape(-1,1), np.array(pred_isNG_list).reshape(-1,1),average='macro')\n",
    "\n",
    "print(val_f1_score, test_f1_score)\n",
    "\n",
    "cm = confusion_matrix(np.array(true_isNG_list).reshape(-1,1), np.array(pred_isNG_list).reshape(-1,1))\n",
    "sns.heatmap(cm, annot=True, cmap='Blues')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = pd.read_csv(\"./preprocess_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1['위험군'] = 0\n",
    "for i in range(len(quality)):\n",
    "    cluster_idx = quality['배정번호'].iloc[i]\n",
    "    data1.loc[data1['배정번호']==cluster_idx, '위험군'] = np.array(total_label[i]).reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quality.to_csv(\"final_quality.csv\",index=False)\n",
    "data1.to_csv(\"final_data.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
